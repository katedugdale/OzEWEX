{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The impact of water storages on flood plains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook generates wofs images before and after a given date. It is meant as a tool for studying floodplains, so only wofs data for days when there were floods are used. To only get the days when the floodplain was in flood, we linked the wofs data to stream gauge data on the time dimension and called only high-flow days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import datacube\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry \n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.helpers import write_geotiff\n",
    "import geopandas as gpd\n",
    "import ipyleaflet as ipyl\n",
    "import ipywidgets as ipyw\n",
    "import json\n",
    "\n",
    "\n",
    "dc = datacube.Datacube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyproj/crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5068aed92c4b40af816565074722f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Map(basemap={'url': 'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', 'max_zoom': 19, 'attrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loading the file and filtering out structures under a certain size:\n",
    "gdf = gpd.read_file('Shapefiles/Northern_Basin.shp')\n",
    "gdf = gdf[gdf.area > 10000]\n",
    "\n",
    "#Converting to EPSG4326:\n",
    "gdf = gdf.to_crs({'init':'epsg:4326'}) #4326 is the tag for lat lon format\n",
    "\n",
    "#Adding a latitude and longitude column:\n",
    "def getXY(pt):\n",
    "    return (pt.x, pt.y)\n",
    "centroidseries = gdf['geometry'].centroid\n",
    "x,y = [list(t) for t in zip(*map(getXY, centroidseries))]\n",
    "gdf = gdf.assign(lon=x)\n",
    "gdf = gdf.assign(lat=y)\n",
    "gdf = gdf.rename(columns={'First50%': 'Construction_date'})\n",
    "gdf['info'] = list(zip(gdf.lat, gdf.lon, gdf.Construction_date))\n",
    "\n",
    "#Converting the data to json\n",
    "data = json.loads(gdf.to_json())\n",
    "\n",
    "map = ipyl.Map(center=[-30, 147], zoom=7)\n",
    "\n",
    "label = ipyw.Label(layout=ipyw.Layout(width='100%'))\n",
    "\n",
    "for feature in data['features']:\n",
    "    feature['properties']['style'] = {\n",
    "        'color': 'grey',\n",
    "        'weight': 1,\n",
    "        'fillColor': 'grey',\n",
    "        'fillOpacity': 0.5\n",
    "    }\n",
    "layer = ipyl.GeoJSON(data=data, hover_style={'fillColor': 'red'})\n",
    "\n",
    "def click_handler(event=None, feature=None, id=None, properties=None):\n",
    "    label.value = str(properties['info'])\n",
    "    \n",
    "    \n",
    "layer.on_click(click_handler)\n",
    "map.add_layer(layer)\n",
    "\n",
    "\n",
    "ipyw.VBox([map, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter user inputs such as lat, lon and construction date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-39015a7fce1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstruction_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "info = label.value.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "info = info.split(\",\")\n",
    "lat, lon, construction_date = info\n",
    "lat = float(lat)\n",
    "lon = float(lon)\n",
    "\n",
    "buffer = 30000 #This is the radius around the lat, lon to display\n",
    "\n",
    "#These are the exceedence values on the x axis of a flow duration curve\n",
    "#Extreme flooding events sometimes mess up the delta image. Take them out by putting a limit on the x-axis. \n",
    "top_flow_percentage = 20 #recommended to set to 20\n",
    "flow_percentage_limit = 0 #recommended to set to 0, but if you want to exclude outlying floods, change to 2. \n",
    "\n",
    "#Where abouts is your csv file with the gauge data located on your computer?\n",
    "csv_file_location = 'Gauges/Condamine-Balonne/Culga_River_Whyenbah.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling wofs data for days when the stream gauge was reading high flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading and organising guage data into a flow duration curve\n",
    "gauge_data = pd.read_csv(csv_file_location,\n",
    "                error_bad_lines = False, skiprows=9, escapechar='#', \n",
    "                         parse_dates=['Timestamp'], #Tells it this column is date format\n",
    "                         index_col=('Timestamp'),\n",
    "                        date_parser=lambda x: pd.to_datetime(x.rsplit('+', 1)[0]))\n",
    "gauge_data = gauge_data.dropna()\n",
    "gauge_data = gauge_data.sort_values('Value')\n",
    "gauge_data['rownumber'] = np.arange(len(gauge_data))\n",
    "gauge_data['Exceedence'] = (1-(gauge_data.rownumber/len(gauge_data)))*100\n",
    "gauge_data = gauge_data.drop(columns='Interpolation Type')\n",
    "gauge_data = gauge_data.drop(columns='Quality Code')\n",
    "\n",
    "# Set up a query which includes ALL satellite data from 1987\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),    \n",
    "         'time': ('1970-01-01', '2019-11-01'), # Change this date to match present day\n",
    "         'crs': 'EPSG:3577'} \n",
    "\n",
    "#Use Dask to load satellite data. Rather than loading images, it loads parameters only. \n",
    "wofs_albers = dc.load(product = 'wofs_albers', \n",
    "                      dask_chunks = {}, \n",
    "                      #group_by='solar_day', #this sometimes messes up the tiles.\n",
    "                      **query)\n",
    "\n",
    "gauge_data_xr = gauge_data.to_xarray()\n",
    "\n",
    "# Merging satellite data with gauge data by timestamp\n",
    "merged_data = gauge_data_xr.interp(Timestamp=wofs_albers.time, method='nearest')\n",
    "\n",
    "# Here is where it takes into account user input for the FDC\n",
    "high_flow = merged_data.where((merged_data.Exceedence < top_flow_percentage) & \n",
    "                              (merged_data.Exceedence > flow_percentage_limit), drop=True)\n",
    "\n",
    "# Get list of dates when gauge was reading high flow\n",
    "date_list = high_flow.time.values\n",
    "\n",
    "print(f'You are about to load {high_flow.time.shape[0]} satellite passes')\n",
    "\n",
    "print(f'lat = {lat}')\n",
    "print(f'lon = {lon}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load wofs images and cloud mask them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the passes that happened during the specified flow parameters\n",
    "high_flow_passes = wofs_albers.sel(time=date_list).compute()\n",
    "\n",
    "# Cloudmask\n",
    "cc = masking.make_mask(high_flow_passes.water, cloud=True)\n",
    "ncloud_pixels = cc.sum(dim=['x', 'y'])\n",
    "npixels_per_slice = (high_flow_passes.water.shape[1] * \n",
    "                     high_flow_passes.water.shape[2])\n",
    "cloud_pixels_fraction = (ncloud_pixels / npixels_per_slice)\n",
    "clear_high_flow_passes = high_flow_passes.water.isel(time=cloud_pixels_fraction < 0.5)\n",
    "\n",
    "#You can un-hash these lines if you want to see a summary image of wofs for all time.\n",
    "#wet = (clear_high_flow_passes == 128).sum(dim='time')\n",
    "#dry = (clear_high_flow_passes == 0).sum(dim='time')\n",
    "#clear = wet + dry\n",
    "#frequency = wet / clear\n",
    "#frequency= frequency.fillna(0) #this is to get rid of the NAs that occur due to mountain shadows\n",
    "#frequency = frequency.where(frequency!=0)\n",
    "#frequency.plot(figsize = (10,8))\n",
    "\n",
    "#print(f'This image is made of {high_flow_passes.time.shape[0]} satellite passes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the wofs data into before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by date into before and after legislation about floodplain harvesting\n",
    "query_dates = clear_high_flow_passes\n",
    "\n",
    "before_construction = query_dates.loc[dict(time=slice('1970-01-01', construction_date))] # Change according to date legislation became effective\n",
    "after_construction = query_dates.loc[dict(time=slice(construction_date, '2020-01-01'))]\n",
    "\n",
    "print(f'The Before image is made of {before_construction.time.shape[0]} satellite passes')\n",
    "print(f'The After image is made of {after_construction.time.shape[0]} satellite passes')\n",
    "\n",
    "#Create parameters for the image\n",
    "wet = (before_construction == 128).sum(dim='time')\n",
    "dry = (before_construction == 0).sum(dim='time')\n",
    "clear = wet + dry\n",
    "frequency_before = wet / clear\n",
    "frequency_before = frequency_before.fillna(0) #this is to get rid of the NAs that occur due to mountain shadows\n",
    "frequency_before = frequency_before.where(frequency_before !=0) #This is to tell it to make areas that were dry 100% of the time white\n",
    "\n",
    "#Plotting the image\n",
    "frequency_before.plot(figsize = (16, 12))\n",
    "plt.axis('off')\n",
    "plt.title('Before')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Create parameters for the image\n",
    "wet = (after_construction == 128).sum(dim='time')\n",
    "dry = (after_construction == 0).sum(dim='time')\n",
    "clear = wet + dry\n",
    "frequency_after = wet / clear\n",
    "frequency_after = frequency_after.fillna(0) #this is to get rid of the NAs that occur due to mountain shadows\n",
    "frequency_after = frequency_after.where(frequency_after !=0) #This is to tell it to make areas that were dry 100% of the time white\n",
    "\n",
    "#Plotting the image\n",
    "frequency_after.plot(figsize = (16, 12))\n",
    "plt.axis('off')\n",
    "plt.title('After')\n",
    "plt.show()\n",
    "\n",
    "delta = frequency_before - frequency_after\n",
    "\n",
    "# Plotting the image\n",
    "delta.plot(figsize = (16, 12), vmin = -0.2, vmax = 0.2, cmap = 'RdYlBu_r')\n",
    "plt.axis('off')\n",
    "plt.title(\"After minus before: Red means water is no longer there and blue means water has appeared there in the 'After' images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Check the distribution of the passes to make sure it's a good representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now take the clear passes and make a pandas dataframe that lists time of clear passes and corresponding gauge value\n",
    "clear_specified_passes_pd = clear_high_flow_passes.time.to_dataframe()\n",
    "clear_specified_passes_pd = clear_specified_passes_pd.rename(columns = {'time': 'date'})#can't have 2 columns called time\n",
    "merged_data_pd = merged_data.to_dataframe()\n",
    "\n",
    "#Merge clear satellite passes with gauge data by the time dimension\n",
    "clear_merged_data = pd.merge(clear_specified_passes_pd, merged_data_pd, left_on= 'time', \n",
    "                            right_index=True, how='inner')\n",
    "clear_merged_data = clear_merged_data.drop(columns='date')\n",
    "clear_merged_data = clear_merged_data.drop(columns='Timestamp')\n",
    "\n",
    "before = clear_merged_data['1987-01-10':'2008-06-01']\n",
    "after  = clear_merged_data['2008-06-01':]\n",
    "\n",
    "# Plot clear satellite passes with a top 20% flowrate over a flow duration curve\n",
    "ax = before.plot(x='Exceedence', marker = 'o', color = 'red', linestyle='None',\n",
    "                y='Value',\n",
    "                logy=True,\n",
    "                title='FDC log showing clear satellite passes of the before and after epochs', figsize = (20,15))\n",
    "after.plot(x='Exceedence', logy=True, y='Value', ax=ax, marker = 'o', color = 'blue', linestyle='None')\n",
    "gauge_data.plot(x='Exceedence', y='Value', logy=True, ax=ax, color = 'blue')\n",
    "ax.legend([\"before\", \"after\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now all that's left to do if find a way to automatically input lat, lon and construction date by simply choosing a water storage from the dea waterbodies shapefile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save wofs delta image as a .tif file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output file name \n",
    "file_name = 'delta_image.tif'\n",
    "\n",
    "# Set up the file for writing\n",
    "delta_image = delta.to_dataset()\n",
    "delta_image.attrs=wofs_albers.attrs\n",
    "\n",
    "# Write GeoTIFF to a location\n",
    "write_geotiff(file_name, delta_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
